{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Policy gradient","metadata":{"id":"AMAe5Yg_2sz4"}},{"cell_type":"code","source":"!pip install git+https://github.com/glmcdona/LuxPythonEnvGym.git\n!pip install kaggle-environments -U","metadata":{"id":"752x7I5B4ZaH","outputId":"8cd8c4af-1974-4883-88c7-01ab9e6b09ba","execution":{"iopub.status.busy":"2021-11-20T11:38:46.184302Z","iopub.execute_input":"2021-11-20T11:38:46.184643Z","iopub.status.idle":"2021-11-20T11:39:12.557897Z","shell.execute_reply.started":"2021-11-20T11:38:46.184556Z","shell.execute_reply":"2021-11-20T11:39:12.556874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Install colabgymrender to display gym environments in Colab\n!python -V\n!pip install gym pyvirtualdisplay > /dev/null 2>&1\n!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n!pip install colabgymrender==1.0.2","metadata":{"id":"wyxFQenM2zIa","outputId":"58f11a50-f86c-402c-b5c1-c0efbe035f24","execution":{"iopub.status.busy":"2021-11-20T11:39:12.560185Z","iopub.execute_input":"2021-11-20T11:39:12.560586Z","iopub.status.idle":"2021-11-20T11:39:57.678928Z","shell.execute_reply.started":"2021-11-20T11:39:12.560539Z","shell.execute_reply":"2021-11-20T11:39:57.678209Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport gym\nfrom gym import wrappers\nimport time\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice\nprint(device)\nstart = time.time()","metadata":{"id":"FMWbaH112sz4","outputId":"c34b80d7-7df3-412a-c581-169b5659076f","execution":{"iopub.status.busy":"2021-11-20T11:39:57.681516Z","iopub.execute_input":"2021-11-20T11:39:57.681766Z","iopub.status.idle":"2021-11-20T11:39:58.964146Z","shell.execute_reply.started":"2021-11-20T11:39:57.681735Z","shell.execute_reply":"2021-11-20T11:39:58.963259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Policy Net","metadata":{"id":"Mu8o6PYglT7w"}},{"cell_type":"code","source":"class PolicyNet(nn.Module):\n    \"\"\"Policy network\"\"\"\n\n    def __init__(self, n_inputs, n_hidden, n_outputs, learning_rate):\n        super(PolicyNet, self).__init__()\n        # network\n        self.hidden_1 = nn.Linear(n_inputs, n_hidden)\n        self.hidden_2 = nn.Linear(n_hidden, n_hidden)\n        self.actor_out = nn.Linear(n_hidden, n_outputs)\n        self.critic_out = nn.Linear(n_hidden, 1)\n        self.rnn = nn.GRU(n_hidden, n_hidden, 2)\n        # training\n        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n        self.saved_actions = []\n        self.rewards = []\n    def forward(self, x):\n        action_prob, state_values = self.predict(x)\n        return action_prob, state_values\n    def predict(self, x, deterministic=False):\n        x = self.hidden_1(x)\n        x = F.relu(x, inplace=False)\n        x, _ = self.rnn(x.view(1, -1, n_hidden))   \n        x = self.hidden_2(x.squeeze())\n        x = F.relu(x, inplace=False)\n        # actor: choses action to take from state s_t \n        # by returning probability of each action\n        action_prob = F.softmax(x, dim=-1)\n\n        # critic: evaluates being in the state s_t\n        state_values = self.critic_out(x)\n\n        # return values for both actor and critic as a tuple of 2 values:\n        # 1. a list with the probability of each action over the action space\n        # 2. the value from state s_t \n        return action_prob, state_values","metadata":{"id":"wvTfsDke2sz-","execution":{"iopub.status.busy":"2021-11-20T12:09:15.272359Z","iopub.execute_input":"2021-11-20T12:09:15.272984Z","iopub.status.idle":"2021-11-20T12:09:15.284507Z","shell.execute_reply.started":"2021-11-20T12:09:15.272934Z","shell.execute_reply":"2021-11-20T12:09:15.2837Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def compute_returns(rewards, discount_factor):\n    \"\"\"Compute discounted returns.\"\"\"\n    returns = np.zeros(len(rewards))\n    returns[-1] = rewards[-1]\n    for t in reversed(range(len(rewards)-1)):\n        returns[t] = rewards[t] + discount_factor * returns[t+1]\n    return returns","metadata":{"id":"QlTn_jfz2sz-","execution":{"iopub.status.busy":"2021-11-20T11:39:58.978927Z","iopub.execute_input":"2021-11-20T11:39:58.97939Z","iopub.status.idle":"2021-11-20T11:39:58.98856Z","shell.execute_reply.started":"2021-11-20T11:39:58.979346Z","shell.execute_reply":"2021-11-20T11:39:58.987816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To start with, our policy will be a rather simple neural network with one hidden layer. We can retrieve the shape of the state space (input) and action space (output) from the environment.","metadata":{"id":"N7zKfnfP2sz-"}},{"cell_type":"code","source":"def smart_transfer_to_nearby(game, team, unit_id, unit, target_type_restriction=None, **kwarg):\n    \"\"\"\n    Smart-transfers from the specified unit to a nearby neighbor. Prioritizes any\n    nearby carts first, then any worker. Transfers the resource type which the unit\n    has most of. Picks which cart/worker based on choosing a target that is most-full\n    but able to take the most amount of resources.\n\n    Args:\n        team ([type]): [description]\n        unit_id ([type]): [description]\n\n    Returns:\n        Action: Returns a TransferAction object, even if the request is an invalid\n                transfer. Use TransferAction.is_valid() to check validity.\n    \"\"\"\n\n    # Calculate how much resources could at-most be transferred\n    resource_type = None\n    resource_amount = 0\n    target_unit = None\n\n    if unit != None:\n        for type, amount in unit.cargo.items():\n            if amount > resource_amount:\n                resource_type = type\n                resource_amount = amount\n\n        # Find the best nearby unit to transfer to\n        unit_cell = game.map.get_cell_by_pos(unit.pos)\n        adjacent_cells = game.map.get_adjacent_cells(unit_cell)\n\n        \n        for c in adjacent_cells:\n            for id, u in c.units.items():\n                # Apply the unit type target restriction\n                if target_type_restriction == None or u.type == target_type_restriction:\n                    if u.team == team:\n                        # This unit belongs to our team, set it as the winning transfer target\n                        # if it's the best match.\n                        if target_unit is None:\n                            target_unit = u\n                        else:\n                            # Compare this unit to the existing target\n                            if target_unit.type == u.type:\n                                # Transfer to the target with the least capacity, but can accept\n                                # all of our resources\n                                if( u.get_cargo_space_left() >= resource_amount and \n                                    target_unit.get_cargo_space_left() >= resource_amount ):\n                                    # Both units can accept all our resources. Prioritize one that is most-full.\n                                    if u.get_cargo_space_left() < target_unit.get_cargo_space_left():\n                                        # This new target it better, it has less space left and can take all our\n                                        # resources\n                                        target_unit = u\n                                    \n                                elif( target_unit.get_cargo_space_left() >= resource_amount ):\n                                    # Don't change targets. Current one is best since it can take all\n                                    # the resources, but new target can't.\n                                    pass\n                                    \n                                elif( u.get_cargo_space_left() > target_unit.get_cargo_space_left() ):\n                                    # Change targets, because neither target can accept all our resources and \n                                    # this target can take more resources.\n                                    target_unit = u\n                            elif u.type == Constants.UNIT_TYPES.CART:\n                                # Transfer to this cart instead of the current worker target\n                                target_unit = u\n    \n    # Build the transfer action request\n    target_unit_id = None\n    if target_unit is not None:\n        target_unit_id = target_unit.id\n\n        # Update the transfer amount based on the room of the target\n        if target_unit.get_cargo_space_left() < resource_amount:\n            resource_amount = target_unit.get_cargo_space_left()\n    \n    return TransferAction(team, unit_id, target_unit_id, resource_type, resource_amount)","metadata":{"id":"ZbiaRY2d8bd-","execution":{"iopub.status.busy":"2021-11-20T11:39:58.989756Z","iopub.execute_input":"2021-11-20T11:39:58.990072Z","iopub.status.idle":"2021-11-20T11:39:59.026099Z","shell.execute_reply.started":"2021-11-20T11:39:58.990044Z","shell.execute_reply":"2021-11-20T11:39:59.025437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile agent_policy.py\nfrom luxai2021.game.match_controller import ActionSequence\nimport sys\nimport time\nfrom functools import partial  # pip install functools\n\nimport numpy as np\nfrom gym import spaces\nimport copy\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport gym\nfrom gym import wrappers\nimport time\nfrom luxai2021.env.agent import Agent\nfrom luxai2021.game.actions import *\nfrom luxai2021.game.game_constants import GAME_CONSTANTS\nfrom luxai2021.game.position import Position\n\n# https://codereview.stackexchange.com/questions/28207/finding-the-closest-point-to-a-list-of-points\ndef closest_node(node, nodes):\n    dist_2 = np.sum((nodes - node) ** 2, axis=1)\n    return np.argmin(dist_2)\ndef furthest_node(node, nodes):\n    dist_2 = np.sum((nodes - node) ** 2, axis=1)\n    return np.argmax(dist_2)\n\ndef smart_transfer_to_nearby(game, team, unit_id, unit, target_type_restriction=None, **kwarg):\n    \"\"\"\n    Smart-transfers from the specified unit to a nearby neighbor. Prioritizes any\n    nearby carts first, then any worker. Transfers the resource type which the unit\n    has most of. Picks which cart/worker based on choosing a target that is most-full\n    but able to take the most amount of resources.\n\n    Args:\n        team ([type]): [description]\n        unit_id ([type]): [description]\n\n    Returns:\n        Action: Returns a TransferAction object, even if the request is an invalid\n                transfer. Use TransferAction.is_valid() to check validity.\n    \"\"\"\n\n    # Calculate how much resources could at-most be transferred\n    resource_type = None\n    resource_amount = 0\n    target_unit = None\n\n    if unit != None:\n        for type, amount in unit.cargo.items():\n            if amount > resource_amount:\n                resource_type = type\n                resource_amount = amount\n\n        # Find the best nearby unit to transfer to\n        unit_cell = game.map.get_cell_by_pos(unit.pos)\n        adjacent_cells = game.map.get_adjacent_cells(unit_cell)\n\n        \n        for c in adjacent_cells:\n            for id, u in c.units.items():\n                # Apply the unit type target restriction\n                if target_type_restriction == None or u.type == target_type_restriction:\n                    if u.team == team:\n                        # This unit belongs to our team, set it as the winning transfer target\n                        # if it's the best match.\n                        if target_unit is None:\n                            target_unit = u\n                        else:\n                            # Compare this unit to the existing target\n                            if target_unit.type == u.type:\n                                # Transfer to the target with the least capacity, but can accept\n                                # all of our resources\n                                if( u.get_cargo_space_left() >= resource_amount and \n                                    target_unit.get_cargo_space_left() >= resource_amount ):\n                                    # Both units can accept all our resources. Prioritize one that is most-full.\n                                    if u.get_cargo_space_left() < target_unit.get_cargo_space_left():\n                                        # This new target it better, it has less space left and can take all our\n                                        # resources\n                                        target_unit = u\n                                    \n                                elif( target_unit.get_cargo_space_left() >= resource_amount ):\n                                    # Don't change targets. Current one is best since it can take all\n                                    # the resources, but new target can't.\n                                    pass\n                                    \n                                elif( u.get_cargo_space_left() > target_unit.get_cargo_space_left() ):\n                                    # Change targets, because neither target can accept all our resources and \n                                    # this target can take more resources.\n                                    target_unit = u\n                            elif u.type == Constants.UNIT_TYPES.CART:\n                                # Transfer to this cart instead of the current worker target\n                                target_unit = u\n    \n    # Build the transfer action request\n    target_unit_id = None\n    if target_unit is not None:\n        target_unit_id = target_unit.id\n\n        # Update the transfer amount based on the room of the target\n        if target_unit.get_cargo_space_left() < resource_amount:\n            resource_amount = target_unit.get_cargo_space_left()\n    \n    return TransferAction(team, unit_id, target_unit_id, resource_type, resource_amount)\n\n########################################################################################################################\n# This is the Agent that you need to design for the competition\n########################################################################################################################\nclass AgentPolicy(Agent):\n    def __init__(self, mode=\"train\", model=None) -> None:\n        \"\"\"\n        Arguments:\n            mode: \"train\" or \"inference\", which controls if this agent is for training or not.\n            model: The pretrained model, or if None it will operate in training mode.\n        \"\"\"\n        super().__init__()\n        self.model = model\n        self.mode = mode\n        \n        self.stats = None\n        self.stats_last_game = None\n\n        # Define action and observation space\n        # They must be gym.spaces objects\n        # Example when using discrete actions:\n        self.actionSpaceMapUnits = [\n            partial(MoveAction, direction=Constants.DIRECTIONS.CENTER),  # This is the do-nothing action\n            partial(MoveAction, direction=Constants.DIRECTIONS.NORTH),\n            partial(MoveAction, direction=Constants.DIRECTIONS.WEST),\n            partial(MoveAction, direction=Constants.DIRECTIONS.SOUTH),\n            partial(MoveAction, direction=Constants.DIRECTIONS.EAST),\n            smart_transfer_to_nearby, # Transfer to nearby\n            SpawnCityAction,\n            #PillageAction,\n        ]\n        self.actionSpaceMapCities = [\n            SpawnWorkerAction,\n            SpawnCartAction,\n            ResearchAction,\n        ]\n\n        self.action_space = spaces.Discrete(max(len(self.actionSpaceMapUnits), len(self.actionSpaceMapCities)))\n        \n\n        # Observation space: (Basic minimum for a miner agent)\n        # Object:\n        #   1x is worker\n        #   1x is cart\n        #   1x is citytile\n        #\n        #   5x direction_nearest_wood\n        #   1x distance_nearest_wood\n        #   1x amount\n        #\n        #   5x direction_nearest_coal\n        #   1x distance_nearest_coal\n        #   1x amount\n        #\n        #   5x direction_nearest_uranium\n        #   1x distance_nearest_uranium\n        #   1x amount\n        #\n        #   5x direction_nearest_city\n        #   1x distance_nearest_city\n        #   1x amount of fuel\n        #\n        #   28x (the same as above, but direction, distance, and amount to the furthest of each)\n        #\n        #   5x direction_nearest_worker\n        #   1x distance_nearest_worker\n        #   1x amount of cargo\n        # Unit:\n        #   1x cargo size\n        # State:\n        #   1x is night\n        #   1x percent of game done\n        #   2x citytile counts [cur player, opponent]\n        #   2x worker counts [cur player, opponent]\n        #   2x cart counts [cur player, opponent]\n        #   1x research points [cur player]\n        #   1x researched coal [cur player]\n        #   1x researched uranium [cur player]\n        self.observation_shape = (3 + 7 * 5 * 2 + 1 + 1 + 1 + 2 + 2 + 2 + 3,)\n        self.observation_space = spaces.Box(low=0, high=1, shape=\n        self.observation_shape, dtype=np.float16)\n\n        self.object_nodes = {}\n\n    def get_agent_type(self):\n        \"\"\"\n        Returns the type of agent. Use AGENT for inference, and LEARNING for training a model.\n        \"\"\"\n        if self.mode == \"train\":\n            return Constants.AGENT_TYPE.LEARNING\n        else:\n            return Constants.AGENT_TYPE.AGENT\n\n    def get_observation(self, game, unit, city_tile, team, is_new_turn):\n        \"\"\"\n        Implements getting a observation from the current game for this unit or city\n        \"\"\"\n        observation_index = 0\n        if is_new_turn:\n            # It's a new turn this event. This flag is set True for only the first observation from each turn.\n            # Update any per-turn fixed observation space that doesn't change per unit/city controlled.\n\n            # Build a list of object nodes by type for quick distance-searches\n            self.object_nodes = {}\n\n            # Add resources\n            for cell in game.map.resources:\n                if cell.resource.type not in self.object_nodes:\n                    self.object_nodes[cell.resource.type] = np.array([[cell.pos.x, cell.pos.y]])\n                else:\n                    self.object_nodes[cell.resource.type] = np.concatenate(\n                        (\n                            self.object_nodes[cell.resource.type],\n                            [[cell.pos.x, cell.pos.y]]\n                        ),\n                        axis=0\n                    )\n\n            # Add your own and opponent units\n            for t in [team, (team + 1) % 2]:\n                for u in game.state[\"teamStates\"][team][\"units\"].values():\n                    key = str(u.type)\n                    if t != team:\n                        key = str(u.type) + \"_opponent\"\n\n                    if key not in self.object_nodes:\n                        self.object_nodes[key] = np.array([[u.pos.x, u.pos.y]])\n                    else:\n                        self.object_nodes[key] = np.concatenate(\n                            (\n                                self.object_nodes[key],\n                                [[u.pos.x, u.pos.y]]\n                            )\n                            , axis=0\n                        )\n\n            # Add your own and opponent cities\n            for city in game.cities.values():\n                for cells in city.city_cells:\n                    key = \"city\"\n                    if city.team != team:\n                        key = \"city_opponent\"\n\n                    if key not in self.object_nodes:\n                        self.object_nodes[key] = np.array([[cells.pos.x, cells.pos.y]])\n                    else:\n                        self.object_nodes[key] = np.concatenate(\n                            (\n                                self.object_nodes[key],\n                                [[cells.pos.x, cells.pos.y]]\n                            )\n                            , axis=0\n                        )\n\n        # Observation space: (Basic minimum for a miner agent)\n        # Object:\n        #   1x is worker\n        #   1x is cart\n        #   1x is citytile\n        #   5x direction_nearest_wood\n        #   1x distance_nearest_wood\n        #   1x amount\n        #\n        #   5x direction_nearest_coal\n        #   1x distance_nearest_coal\n        #   1x amount\n        #\n        #   5x direction_nearest_uranium\n        #   1x distance_nearest_uranium\n        #   1x amount\n        #\n        #   5x direction_nearest_city\n        #   1x distance_nearest_city\n        #   1x amount of fuel\n        #\n        #   5x direction_nearest_worker\n        #   1x distance_nearest_worker\n        #   1x amount of cargo\n        #\n        #   28x (the same as above, but direction, distance, and amount to the furthest of each)\n        #\n        # Unit:\n        #   1x cargo size\n        # State:\n        #   1x is night\n        #   1x percent of game done\n        #   2x citytile counts [cur player, opponent]\n        #   2x worker counts [cur player, opponent]\n        #   2x cart counts [cur player, opponent]\n        #   1x research points [cur player]\n        #   1x researched coal [cur player]\n        #   1x researched uranium [cur player]\n        obs = np.zeros(self.observation_shape)\n        \n        # Update the type of this object\n        #   1x is worker\n        #   1x is cart\n        #   1x is citytile\n        observation_index = 0\n        if unit is not None:\n            if unit.type == Constants.UNIT_TYPES.WORKER:\n                obs[observation_index] = 1.0 # Worker\n            else:\n                obs[observation_index+1] = 1.0 # Cart\n        if city_tile is not None:\n            obs[observation_index+2] = 1.0 # CityTile\n        observation_index += 3\n        \n        pos = None\n        if unit is not None:\n            pos = unit.pos\n        else:\n            pos = city_tile.pos\n\n        if pos is None:\n            observation_index += 7 * 5 * 2\n        else:\n            # Encode the direction to the nearest objects\n            #   5x direction_nearest\n            #   1x distance\n            for distance_function in [closest_node, furthest_node]:\n                for key in [\n                    Constants.RESOURCE_TYPES.WOOD,\n                    Constants.RESOURCE_TYPES.COAL,\n                    Constants.RESOURCE_TYPES.URANIUM,\n                    \"city\",\n                    str(Constants.UNIT_TYPES.WORKER)]:\n                    # Process the direction to and distance to this object type\n\n                    # Encode the direction to the nearest object (excluding itself)\n                    #   5x direction\n                    #   1x distance\n                    if key in self.object_nodes:\n                        if (\n                                (key == \"city\" and city_tile is not None) or\n                                (unit is not None and str(unit.type) == key and len(game.map.get_cell_by_pos(unit.pos).units) <= 1 )\n                        ):\n                            # Filter out the current unit from the closest-search\n                            closest_index = closest_node((pos.x, pos.y), self.object_nodes[key])\n                            filtered_nodes = np.delete(self.object_nodes[key], closest_index, axis=0)\n                        else:\n                            filtered_nodes = self.object_nodes[key]\n\n                        if len(filtered_nodes) == 0:\n                            # No other object of this type\n                            obs[observation_index + 5] = 1.0\n                        else:\n                            # There is another object of this type\n                            closest_index = distance_function((pos.x, pos.y), filtered_nodes)\n\n                            if closest_index is not None and closest_index >= 0:\n                                closest = filtered_nodes[closest_index]\n                                closest_position = Position(closest[0], closest[1])\n                                direction = pos.direction_to(closest_position)\n                                mapping = {\n                                    Constants.DIRECTIONS.CENTER: 0,\n                                    Constants.DIRECTIONS.NORTH: 1,\n                                    Constants.DIRECTIONS.WEST: 2,\n                                    Constants.DIRECTIONS.SOUTH: 3,\n                                    Constants.DIRECTIONS.EAST: 4,\n                                }\n                                obs[observation_index + mapping[direction]] = 1.0  # One-hot encoding direction\n\n                                # 0 to 1 distance\n                                distance = pos.distance_to(closest_position)\n                                obs[observation_index + 5] = min(distance / 20.0, 1.0)\n\n                                # 0 to 1 value (amount of resource, cargo for unit, or fuel for city)\n                                if key == \"city\":\n                                    # City fuel as % of upkeep for 200 turns\n                                    c = game.cities[game.map.get_cell_by_pos(closest_position).city_tile.city_id]\n                                    obs[observation_index + 6] = min(\n                                        c.fuel / (c.get_light_upkeep() * 200.0),\n                                        1.0\n                                    )\n                                elif key in [Constants.RESOURCE_TYPES.WOOD, Constants.RESOURCE_TYPES.COAL,\n                                             Constants.RESOURCE_TYPES.URANIUM]:\n                                    # Resource amount\n                                    obs[observation_index + 6] = min(\n                                        game.map.get_cell_by_pos(closest_position).resource.amount / 500,\n                                        1.0\n                                    )\n                                else:\n                                    # Unit cargo\n                                    obs[observation_index + 6] = min(\n                                        next(iter(game.map.get_cell_by_pos(\n                                            closest_position).units.values())).get_cargo_space_left() / 100,\n                                        1.0\n                                    )\n\n                    observation_index += 7\n\n        if unit is not None:\n            # Encode the cargo space\n            #   1x cargo size\n            obs[observation_index] = unit.get_cargo_space_left() / GAME_CONSTANTS[\"PARAMETERS\"][\"RESOURCE_CAPACITY\"][\n                \"WORKER\"]\n            observation_index += 1\n        else:\n            observation_index += 1\n\n        # Game state observations\n\n        #   1x is night\n        obs[observation_index] = game.is_night()\n        observation_index += 1\n\n        #   1x percent of game done\n        obs[observation_index] = game.state[\"turn\"] / GAME_CONSTANTS[\"PARAMETERS\"][\"MAX_DAYS\"]\n        observation_index += 1\n\n        #   2x citytile counts [cur player, opponent]\n        #   2x worker counts [cur player, opponent]\n        #   2x cart counts [cur player, opponent]\n        max_count = 30\n        for key in [\"city\", str(Constants.UNIT_TYPES.WORKER), str(Constants.UNIT_TYPES.CART)]:\n            if key in self.object_nodes:\n                obs[observation_index] = len(self.object_nodes[key]) / max_count\n            if (key + \"_opponent\") in self.object_nodes:\n                obs[observation_index + 1] = len(self.object_nodes[(key + \"_opponent\")]) / max_count\n            observation_index += 2\n\n        #   1x research points [cur player]\n        #   1x researched coal [cur player]\n        #   1x researched uranium [cur player]\n        obs[observation_index] = game.state[\"teamStates\"][team][\"researchPoints\"] / 200.0\n        obs[observation_index+1] = float(game.state[\"teamStates\"][team][\"researched\"][\"coal\"])\n        obs[observation_index+2] = float(game.state[\"teamStates\"][team][\"researched\"][\"uranium\"])\n\n        return obs\n\n    def action_code_to_action(self, action_code, game, unit=None, city_tile=None, team=None):\n        \"\"\"\n        Takes an action in the environment according to actionCode:\n            actionCode: Index of action to take into the action array.\n        Returns: An action.\n        \"\"\"\n        # Map actionCode index into to a constructed Action object\n        try:\n            x = None\n            y = None\n            if city_tile is not None:\n                x = city_tile.pos.x\n                y = city_tile.pos.y\n            elif unit is not None:\n                x = unit.pos.x\n                y = unit.pos.y\n            \n            if city_tile != None:\n                action =  self.actionSpaceMapCities[action_code%len(self.actionSpaceMapCities)](\n                    game=game,\n                    unit_id=unit.id if unit else None,\n                    unit=unit,\n                    city_id=city_tile.city_id if city_tile else None,\n                    citytile=city_tile,\n                    team=team,\n                    x=x,\n                    y=y\n                )\n\n                # If the city action is invalid, default to research action automatically\n                if not action.is_valid(game, actions_validated=[]):\n                    action = ResearchAction(\n                        game=game,\n                        unit_id=unit.id if unit else None,\n                        unit=unit,\n                        city_id=city_tile.city_id if city_tile else None,\n                        citytile=city_tile,\n                        team=team,\n                        x=x,\n                        y=y\n                    )\n            else:\n                action =  self.actionSpaceMapUnits[action_code%len(self.actionSpaceMapUnits)](\n                    game=game,\n                    unit_id=unit.id if unit else None,\n                    unit=unit,\n                    city_id=city_tile.city_id if city_tile else None,\n                    citytile=city_tile,\n                    team=team,\n                    x=x,\n                    y=y\n                )\n            \n            return action\n        except Exception as e:\n            # Not a valid action\n            print(e)\n            return None\n\n    def take_action(self, action_code, game, unit=None, city_tile=None, team=None):\n        \"\"\"\n        Takes an action in the environment according to actionCode:\n            actionCode: Index of action to take into the action array.\n        \"\"\"\n        action = self.action_code_to_action(action_code, game, unit, city_tile, team)\n        self.match_controller.take_action(action)\n    \n    def game_start(self, game):\n        \"\"\"\n        This funciton is called at the start of each game. Use this to\n        reset and initialize per game. Note that self.team may have\n        been changed since last game. The game map has been created\n        and starting units placed.\n\n        Args:\n            game ([type]): Game.\n        \"\"\"\n        self.last_generated_fuel = game.stats[\"teamStats\"][self.team][\"fuelGenerated\"]\n        self.last_resources_collected = copy.deepcopy(game.stats[\"teamStats\"][self.team][\"resourcesCollected\"])\n        if self.stats != None:\n            self.stats_last_game =  self.stats\n        self.stats = {\n            \"rew/r_total\": 0,\n            \"rew/r_wood\": 0,\n            \"rew/r_coal\": 0,\n            \"rew/r_uranium\": 0,\n            \"rew/r_research\": 0,\n            \"rew/r_city_tiles_end\": 0,\n            \"rew/r_fuel_collected\":0,\n            \"rew/r_units\":0,\n            \"rew/r_city_tiles\":0,\n            \"game/turns\": 0,\n            \"game/research\": 0,\n            \"game/unit_count\": 0,\n            \"game/cart_count\": 0,\n            \"game/city_count\": 0,\n            \"game/city_tiles\": 0,\n            \"game/wood_rate_mined\": 0,\n            \"game/coal_rate_mined\": 0,\n            \"game/uranium_rate_mined\": 0,\n        }\n        self.is_last_turn = False\n\n        # Calculate starting map resources\n        type_map = {\n            Constants.RESOURCE_TYPES.WOOD: \"WOOD\",\n            Constants.RESOURCE_TYPES.COAL: \"COAL\",\n            Constants.RESOURCE_TYPES.URANIUM: \"URANIUM\",\n        }\n\n        self.fuel_collected_last = 0\n        self.fuel_start = {}\n        self.fuel_last = {}\n        for type, type_upper in type_map.items():\n            self.fuel_start[type] = 0\n            self.fuel_last[type] = 0\n            for c in game.map.resources_by_type[type]:\n                self.fuel_start[type] += c.resource.amount * game.configs[\"parameters\"][\"RESOURCE_TO_FUEL_RATE\"][type_upper]\n\n        self.research_last = 0\n        self.units_last = 0\n        self.city_tiles_last = 0\n\n    def get_reward(self, game, is_game_finished, is_new_turn, is_game_error):\n        \"\"\"\n        Returns the reward function for this step of the game.\n        \"\"\"\n        if is_game_error:\n            # Game environment step failed, assign a game lost reward to not incentivise this\n            print(\"Game failed due to error\")\n            return -1.0\n\n        if not is_new_turn and not is_game_finished:\n            # Only apply rewards at the start of each turn\n            return 0\n\n        # Get some basic stats\n        unit_count = len(game.state[\"teamStates\"][self.team % 2][\"units\"])\n        cart_count = 0\n        for id, u in game.state[\"teamStates\"][self.team % 2][\"units\"].items():\n            if u.type == Constants.UNIT_TYPES.CART:\n                cart_count += 1\n\n        unit_count_opponent = len(game.state[\"teamStates\"][(self.team + 1) % 2][\"units\"])\n        research = min(game.state[\"teamStates\"][self.team][\"researchPoints\"], 200.0) # Cap research points at 200\n        city_count = 0\n        city_count_opponent = 0\n        city_tile_count = 0\n        city_tile_count_opponent = 0\n        for city in game.cities.values():\n            if city.team == self.team:\n                city_count += 1\n            else:\n                city_count_opponent += 1\n\n            for cell in city.city_cells:\n                if city.team == self.team:\n                    city_tile_count += 1\n                else:\n                    city_tile_count_opponent += 1\n        \n        # Basic stats\n        self.stats[\"game/research\"] = research\n        self.stats[\"game/city_tiles\"] = city_tile_count\n        self.stats[\"game/city_count\"] = city_count\n        self.stats[\"game/unit_count\"] = unit_count\n        self.stats[\"game/cart_count\"] = cart_count\n        self.stats[\"game/turns\"] = game.state[\"turn\"]\n\n        rewards = {}\n\n        # Give up to 1.0 reward for each resource based on % of total mined.\n        type_map = {\n            Constants.RESOURCE_TYPES.WOOD: \"WOOD\",\n            Constants.RESOURCE_TYPES.COAL: \"COAL\",\n            Constants.RESOURCE_TYPES.URANIUM: \"URANIUM\",\n        }\n        fuel_now = {}\n        for type, type_upper in type_map.items():\n            fuel_now = game.stats[\"teamStats\"][self.team][\"resourcesCollected\"][type] * game.configs[\"parameters\"][\"RESOURCE_TO_FUEL_RATE\"][type_upper]\n            rewards[\"rew/r_%s\" % type] = (fuel_now - self.fuel_last[type]) / self.fuel_start[type]\n            self.stats[\"game/%s_rate_mined\" % type] = fuel_now / self.fuel_start[type]\n            self.fuel_last[type] = fuel_now\n        \n        # Give more incentive for coal and uranium\n        rewards[\"rew/r_%s\" % Constants.RESOURCE_TYPES.COAL] *= 2\n        rewards[\"rew/r_%s\" % Constants.RESOURCE_TYPES.URANIUM] *= 4\n        \n        # Give a reward based on amount of fuel collected. 1.0 reward for each 20K fuel gathered.\n        fuel_collected = game.stats[\"teamStats\"][self.team][\"fuelGenerated\"]\n        rewards[\"rew/r_fuel_collected\"] = ( (fuel_collected - self.fuel_collected_last) / 20000 )\n        self.fuel_collected_last = fuel_collected\n\n        # Give a reward for unit creation/death. 0.05 reward per unit.\n        rewards[\"rew/r_units\"] = (unit_count - self.units_last) * 0.2\n        self.units_last = unit_count\n\n        # Give a reward for unit creation/death. 0.1 reward per city.\n        rewards[\"rew/r_city_tiles\"] = (city_tile_count - self.city_tiles_last) * 0.2\n        self.city_tiles_last = city_tile_count\n\n        # Tiny reward for research to help. Up to 0.5 reward for this.\n        rewards[\"rew/r_research\"] = (research - self.research_last) / (200 * 2)\n        self.research_last = research\n        \n        # Give a reward up to around 50.0 based on number of city tiles at the end of the game\n        rewards[\"rew/r_city_tiles_end\"] = 0\n        if is_game_finished:\n            self.is_last_turn = True\n            rewards[\"rew/r_city_tiles_end\"] = city_tile_count\n        \n        \n        # Update the stats and total reward\n        reward = 0\n        for name, value in rewards.items():\n            self.stats[name] += value\n            reward += value\n        self.stats[\"rew/r_total\"] += reward\n\n        # Print the final game stats sometimes\n        if is_game_finished and random.random() <= 0.15:\n            stats_string = []\n            for key, value in self.stats.items():\n                stats_string.append(\"%s=%.2f\" % (key, value))\n            print(\",\".join(stats_string))\n\n\n        return reward\n        \n    \n\n    def process_turn(self, game, team):\n        \"\"\"\n        Decides on a set of actions for the current turn. Not used in training, only inference.\n        Returns: Array of actions to perform.\n        \"\"\"\n        start_time = time.time()\n        actions = []\n        new_turn = True\n\n        # Inference the model per-unit\n        units = game.state[\"teamStates\"][team][\"units\"].values()\n        for unit in units:\n            if unit.can_act():\n                obs = self.get_observation(game, unit, None, unit.team, new_turn)\n                obs = torch.from_numpy(obs).float().view(1, 85)\n                action_code, _states = self.model.predict(obs, deterministic=False)\n                if action_code is not None:\n                    actions.append(\n                        self.action_code_to_action(action_code, game=game, unit=unit, city_tile=None, team=unit.team))\n                new_turn = False\n\n        # Inference the model per-city\n        cities = game.cities.values()\n        for city in cities:\n            if city.team == team:\n                for cell in city.city_cells:\n                    city_tile = cell.city_tile\n                    if city_tile.can_act():\n                        obs = self.get_observation(game, None, city_tile, city.team, new_turn)\n                        obs = torch.from_numpy(obs).float().view(1, 85)\n                        action_code, _states = self.model.predict(obs, deterministic=False)\n                        if action_code is not None:\n                            actions.append(\n                                self.action_code_to_action(action_code, game=game, unit=None, city_tile=city_tile,\n                                                           team=city.team))\n                        new_turn = False\n\n        time_taken = time.time() - start_time\n        if time_taken > 0.5:  # Warn if larger than 0.5 seconds.\n            print(\"WARNING: Inference took %.3f seconds for computing actions. Limit is 1 second.\" % time_taken, file=sys.stderr)\n\n        return actions","metadata":{"id":"Z2VDHl9v-dh-","outputId":"32f3e149-75aa-4818-a43c-68cf74e2f7ee","execution":{"iopub.status.busy":"2021-11-20T11:39:59.029049Z","iopub.execute_input":"2021-11-20T11:39:59.029428Z","iopub.status.idle":"2021-11-20T11:39:59.081421Z","shell.execute_reply.started":"2021-11-20T11:39:59.029383Z","shell.execute_reply":"2021-11-20T11:39:59.080717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Run","metadata":{"id":"Zqh5nd9VlbsK"}},{"cell_type":"code","source":"import argparse\nimport glob\nimport os\nimport sys\nimport random\n\nfrom agent_policy import AgentPolicy\nfrom luxai2021.env.agent import Agent\nfrom luxai2021.env.lux_env import LuxEnvironment, SaveReplayAndModelCallback\nfrom luxai2021.game.constants import LuxMatchConfigs_Default\nfrom gym import spaces\nfrom functools import partial \nfrom luxai2021.env.agent import Agent, AgentFromStdInOut\nfrom luxai2021.game.actions import *\nfrom luxai2021.game.game_constants import GAME_CONSTANTS\nfrom luxai2021.game.position import Position\n\n# Run a training job\nconfigs = LuxMatchConfigs_Default\n\n# Create a default opponent agent\nopponent = Agent()\nplayer = AgentPolicy(mode=\"train\")\n# Create a RL agent in training mode\n\n# Train the model\nenv = LuxEnvironment(configs=configs,\n                         learning_agent=player,\n                         opponent_agent=opponent)\n\n\nobservation_shape = 3 + 7 * 5 * 2 + 1 + 1 + 1 + 2 + 2 + 2 + 3\n#observation_space = spaces.Box(low=0, high=1, shape=observation_shape, dtype=np.float16)\nn_inputs = observation_shape\nn_hidden = 64\nactionSpaceMapUnits = [\n  partial(MoveAction, direction=Constants.DIRECTIONS.CENTER),  # This is the do-nothing action\n  partial(MoveAction, direction=Constants.DIRECTIONS.NORTH),\n  partial(MoveAction, direction=Constants.DIRECTIONS.WEST),\n  partial(MoveAction, direction=Constants.DIRECTIONS.SOUTH),\n  partial(MoveAction, direction=Constants.DIRECTIONS.EAST),\n  smart_transfer_to_nearby, # Transfer to nearby\n  SpawnCityAction,\n  #PillageAction,\n]\nactionSpaceMapCities = [\n  SpawnWorkerAction,\n  SpawnCartAction,\n  ResearchAction,\n]\n\naction_space = spaces.Discrete(max(len(actionSpaceMapUnits), len(actionSpaceMapCities)))\nn_outputs = max(len(actionSpaceMapUnits), len(actionSpaceMapCities))\n\nprint('state shape:', n_inputs)\nprint('action shape:', n_outputs)\n\n# training settings\nclip = 0.2\nnum_episodes = 50000\nrollout_limit = 100000 # max rollout length\ndiscount_factor = 0.995 # reward discount factor (gamma), 1.0 = no discount\nlearning_rate = 0.002 # you know this by now\nval_freq = 100 # validation frequency\n\n# setup policy network\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(device)\npolicy = PolicyNet(n_inputs, n_hidden, n_outputs, learning_rate)\n\n# train policy network\n\ntry:\n    training_rewards, losses = [], []\n    print('start training')\n    for i in range(num_episodes):\n        rollout = []\n        rewards = []\n        saved_actions = []\n        batch_log_probs = []\n        policy_losses = [] # list to save actor (policy) loss\n        value_losses = [] # list to save critic (value) loss\n        entropy_losses = []\n        policy.optimizer.zero_grad()\n\n        state = env.reset()\n        ep_reward = 0\n        done = False\n        while (not done):\n            # select action from policy\n            saved_action = policy(torch.from_numpy(np.atleast_2d(state)).float())\n            a_prob, state_value = saved_action\n            a = torch.multinomial(a_prob, num_samples=1).squeeze().cpu().numpy()\n            state, reward, done, _ = env.step(a)\n            rewards.append(reward)\n\n            batch_log_probs.append(a_prob.detach())\n            saved_actions.append(saved_action)\n            policy.rewards.append(reward)\n        #print()\n        batch_log_probs = torch.stack(batch_log_probs)\n        returns = compute_returns(rewards, discount_factor)\n        returns = torch.tensor(returns, dtype=torch.float)\n        returns = (returns - returns.mean()) / (returns.std())\n        for (log_prob, value), R in zip(saved_actions, returns):\n            ratios = torch.exp(log_prob - batch_log_probs.detach())\n            surr1 = ratios * R\n            surr2 = torch.clamp(ratios, 1 - clip, 1 + clip) * R\n            # calculate actor (policy) loss \n            policy_loss = -torch.min(surr1, surr2).mean()\n            policy_losses.append(policy_loss)\n            # calculate critic (value) loss using L1 smooth loss\n            value_loss = nn.MSELoss()(value, torch.tensor([R])).view(1, 1)\n            value_losses.append(value_loss.view(1, 1))\n            entropy_loss = -torch.mean(-log_prob)\n            entropy_losses.append(entropy_loss)\n            # Calculate actor and critic losses.\n            # NOTE: we take the negative min of the surrogate losses because we're trying to maximize\n            # the performance function, but Adam minimizes the loss. So minimizing the negative\n            # performance function maximizes it.\n        loss = torch.stack(policy_losses).sum() + 0.01 * torch.stack(entropy_losses).sum() + 0.5 * torch.stack(value_losses).sum()\n        loss.backward()\n        policy.optimizer.step()\n        # bookkeeping\n        training_rewards.append(sum(policy.rewards))\n        losses.append(loss.item())\n        del policy.rewards[:]\n        del policy.saved_actions[:]\n        # print\n        if (i+1) % val_freq == 0:\n            # validation\n            validation_rewards = []\n            for _ in range(10):\n                s = env.reset()\n                reward = 0\n                done = False\n                while (not done):\n                    with torch.no_grad():\n                        a_prob, _ = policy(torch.from_numpy(np.atleast_2d(s)).float())\n                        a = a_prob.argmax().item()\n                    s, r, done, _ = env.step(a)\n                    reward += r\n                validation_rewards.append(reward)\n            print('{:4d}. mean training reward: {:6.2f}, mean validation reward: {:6.2f}, mean loss: {:7.4f}'.format(i+1, np.mean(training_rewards[-val_freq:]), np.mean(validation_rewards), np.mean(losses[-val_freq:])))\n    print('done')\n    end = time.time()\n    print(end - start)\nexcept KeyboardInterrupt:\n    print('interrupt')    ","metadata":{"id":"seCct_er4mCW","outputId":"a7067c67-788b-47e4-dc7b-74fcf4c90924","execution":{"iopub.status.busy":"2021-11-20T12:09:23.337404Z","iopub.execute_input":"2021-11-20T12:09:23.337828Z","iopub.status.idle":"2021-11-20T12:10:04.361714Z","shell.execute_reply.started":"2021-11-20T12:09:23.337796Z","shell.execute_reply":"2021-11-20T12:10:04.360862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(policy, \"tensor.pt\")  ","metadata":{"id":"KzV1SjqM0RCG","execution":{"iopub.status.busy":"2021-11-20T11:39:59.652638Z","iopub.status.idle":"2021-11-20T11:39:59.653362Z","shell.execute_reply.started":"2021-11-20T11:39:59.653031Z","shell.execute_reply":"2021-11-20T11:39:59.653064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plot results\ndef moving_average(a, n=10) :\n    ret = np.cumsum(a, dtype=float)\n    ret[n:] = ret[n:] - ret[:-n]\n    return ret / n\n\nplt.figure(figsize=(16,6))\nplt.subplot(211)\nplt.plot(range(1, len(training_rewards)+1), training_rewards, label='training reward')\n#plt.plot(moving_average(training_rewards))\nplt.xlabel('episode'); plt.ylabel('reward')\nplt.xlim((0, len(training_rewards)))\nplt.legend(loc=4); plt.grid()\nplt.subplot(211)\nplt.plot(range(1, len(validation_rewards)+1), validation_rewards, label='validation_reward')\n#plt.plot(moving_average(validation_rewards))\nplt.xlabel('episode'); plt.ylabel('reward')\nplt.xlim((0, len(validation_rewards)))\nplt.legend(loc=4); plt.grid()\nplt.subplot(212)\nplt.plot(range(1, len(losses)+1), losses, label='loss')\nplt.plot(moving_average(losses))\nplt.xlabel('episode'); plt.ylabel('loss')\nplt.xlim((0, len(losses)))\nplt.legend(loc=4); plt.grid()\nplt.tight_layout(); plt.show()","metadata":{"id":"20eqHolWLmdQ","outputId":"ea53f531-5471-4ae9-f9b7-d2f5a376a15f","execution":{"iopub.status.busy":"2021-11-20T11:39:59.654732Z","iopub.status.idle":"2021-11-20T11:39:59.655021Z","shell.execute_reply.started":"2021-11-20T11:39:59.654874Z","shell.execute_reply":"2021-11-20T11:39:59.65489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nThis downloads two required python package dependencies that are not pre-installed\nby Kaggle yet.\n\nThis places the following two packages in the current working directory:\n    luxai2021\n    stable_baselines3\n\"\"\"\n\nimport os\nimport shutil\nimport subprocess\nimport tempfile\n\ndef localize_package(git, branch, folder):\n    if os.path.exists(folder):\n        print(\"Already localized %s\" % folder)\n    else:\n        # https://stackoverflow.com/questions/51239168/how-to-download-single-file-from-a-git-repository-using-python\n        # Create temporary dir\n        t = tempfile.mkdtemp()\n\n        args = ['git', 'clone', '--depth=1', git, t, '-b', branch]\n        res = subprocess.Popen(args, stdout=subprocess.PIPE)\n        output, _error = res.communicate()\n\n        if not _error:\n            print(output)\n        else:\n            print(_error)\n        \n        # Copy desired file from temporary dir\n        shutil.move(os.path.join(t, folder), '.')\n        # Remove temporary dir\n        shutil.rmtree(t, ignore_errors=True)\n\nlocalize_package('https://github.com/glmcdona/LuxPythonEnvGym.git', 'main', 'luxai2021')\nlocalize_package('https://github.com/glmcdona/LuxPythonEnvGym.git', 'main', 'kaggle_submissions')\nlocalize_package('https://github.com/DLR-RM/stable-baselines3.git', 'master', 'stable_baselines3')","metadata":{"id":"fNTInXQ9vZtA","outputId":"9bc2e5d0-7478-45ed-8c09-ea1209496789","execution":{"iopub.status.busy":"2021-11-20T11:39:59.655862Z","iopub.status.idle":"2021-11-20T11:39:59.656163Z","shell.execute_reply.started":"2021-11-20T11:39:59.655996Z","shell.execute_reply":"2021-11-20T11:39:59.656012Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile main_lux-ai-2021.py\n\nfrom luxai2021.env.agent import AgentFromStdInOut\nfrom luxai2021.env.lux_env import LuxEnvironment\nfrom luxai2021.game.constants import LuxMatchConfigs_Default\nfrom agent_policy import AgentPolicy\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport gym\nfrom gym import wrappers\nimport time\nimport torch\n\nclass PolicyNet(nn.Module):\n    \"\"\"Policy network\"\"\"\n\n    def __init__(self, n_inputs, n_hidden, n_outputs, learning_rate):\n        super(PolicyNet, self).__init__()\n        # network\n        self.hidden_1 = nn.Linear(n_inputs, n_hidden)\n        self.hidden_2 = nn.Linear(n_hidden, n_hidden)\n        self.actor_out = nn.Linear(n_hidden, n_outputs)\n        self.critic_out = nn.Linear(n_hidden, 1)\n        self.rnn = nn.GRU(n_hidden, n_hidden, 2)\n        # training\n        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n        self.saved_actions = []\n        self.rewards = []\n    def forward(self, x):\n        action_prob, state_values = self.predict(x)\n        return action_prob, state_values\n    def predict(self, x, deterministic=False):\n        x = self.hidden_1(x)\n        x = F.relu(x, inplace=False)\n        x, _ = self.rnn(x.view(1, -1, n_hidden))   \n        x = self.hidden_2(x.squeeze())\n        x = F.relu(x, inplace=False)\n        # actor: choses action to take from state s_t \n        # by returning probability of each action\n        action_prob = F.softmax(x, dim=-1)\n\n        # critic: evaluates being in the state s_t\n        state_values = self.critic_out(x)\n\n        # return values for both actor and critic as a tuple of 2 values:\n        # 1. a list with the probability of each action over the action space\n        # 2. the value from state s_t \n        return action_prob, state_values\n\ndef compute_returns(rewards, discount_factor):\n    \"\"\"Compute discounted returns.\"\"\"\n    returns = np.zeros(len(rewards))\n    returns[-1] = rewards[-1]\n    for t in reversed(range(len(rewards)-1)):\n        returns[t] = rewards[t] + discount_factor * returns[t+1]\n    return returns\n\ndef smart_transfer_to_nearby(game, team, unit_id, unit, target_type_restriction=None, **kwarg):\n    \"\"\"\n    Smart-transfers from the specified unit to a nearby neighbor. Prioritizes any\n    nearby carts first, then any worker. Transfers the resource type which the unit\n    has most of. Picks which cart/worker based on choosing a target that is most-full\n    but able to take the most amount of resources.\n\n    Args:\n        team ([type]): [description]\n        unit_id ([type]): [description]\n\n    Returns:\n        Action: Returns a TransferAction object, even if the request is an invalid\n                transfer. Use TransferAction.is_valid() to check validity.\n    \"\"\"\n\n    # Calculate how much resources could at-most be transferred\n    resource_type = None\n    resource_amount = 0\n    target_unit = None\n\n    if unit != None:\n        for type, amount in unit.cargo.items():\n            if amount > resource_amount:\n                resource_type = type\n                resource_amount = amount\n\n        # Find the best nearby unit to transfer to\n        unit_cell = game.map.get_cell_by_pos(unit.pos)\n        adjacent_cells = game.map.get_adjacent_cells(unit_cell)\n\n        \n        for c in adjacent_cells:\n            for id, u in c.units.items():\n                # Apply the unit type target restriction\n                if target_type_restriction == None or u.type == target_type_restriction:\n                    if u.team == team:\n                        # This unit belongs to our team, set it as the winning transfer target\n                        # if it's the best match.\n                        if target_unit is None:\n                            target_unit = u\n                        else:\n                            # Compare this unit to the existing target\n                            if target_unit.type == u.type:\n                                # Transfer to the target with the least capacity, but can accept\n                                # all of our resources\n                                if( u.get_cargo_space_left() >= resource_amount and \n                                    target_unit.get_cargo_space_left() >= resource_amount ):\n                                    # Both units can accept all our resources. Prioritize one that is most-full.\n                                    if u.get_cargo_space_left() < target_unit.get_cargo_space_left():\n                                        # This new target it better, it has less space left and can take all our\n                                        # resources\n                                        target_unit = u\n                                    \n                                elif( target_unit.get_cargo_space_left() >= resource_amount ):\n                                    # Don't change targets. Current one is best since it can take all\n                                    # the resources, but new target can't.\n                                    pass\n                                    \n                                elif( u.get_cargo_space_left() > target_unit.get_cargo_space_left() ):\n                                    # Change targets, because neither target can accept all our resources and \n                                    # this target can take more resources.\n                                    target_unit = u\n                            elif u.type == Constants.UNIT_TYPES.CART:\n                                # Transfer to this cart instead of the current worker target\n                                target_unit = u\n    \n    # Build the transfer action request\n    target_unit_id = None\n    if target_unit is not None:\n        target_unit_id = target_unit.id\n\n        # Update the transfer amount based on the room of the target\n        if target_unit.get_cargo_space_left() < resource_amount:\n            resource_amount = target_unit.get_cargo_space_left()\n    \n    return TransferAction(team, unit_id, target_unit_id, resource_type, resource_amount)\n\nif __name__ == \"__main__\":\n    \"\"\"\n    This is a kaggle submission, so we don't use command-line args\n    and assume the model is in model.zip in the current folder.\n    \"\"\"\n    # Tool to run this against itself locally:\n    # \"lux-ai-2021 --seed=100 main_lux-ai-2021.py main_lux-ai-2021.py --maxtime 10000\"\n\n    # Run a kaggle submission with the specified model\n    configs = LuxMatchConfigs_Default\n\n    # Load the saved model\n    #model_id = 5403\n    #total_steps = int(48e6)\n    #model = PPO.load(f\"models/rl_model_{model_id}_{total_steps}_steps.zip\")\n    model = torch.load(\"tensor.pt\")\n    \n    # Create a kaggle-remote opponent agent\n    opponent = AgentFromStdInOut()\n\n    # Create a RL agent in inference mode\n    player = AgentPolicy(mode=\"inference\", model=model)\n\n    # Run the environment\n    env = LuxEnvironment(configs, player, opponent)\n    env.reset()  # This will automatically run the game since there is\n    # no controlling learning agent.\n","metadata":{"execution":{"iopub.status.busy":"2021-11-20T11:39:59.657374Z","iopub.status.idle":"2021-11-20T11:39:59.65769Z","shell.execute_reply.started":"2021-11-20T11:39:59.657521Z","shell.execute_reply":"2021-11-20T11:39:59.657537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile main.py\n\"\"\"\nCopied and moidified from here:\nhttps://github.com/Lux-AI-Challenge/Lux-Design-2021/blob/master/kits/cpp/simple/main.py\n\nIn a kaggle-environment submission, main.py is loaded into memory and the last function is\nsearched for. The last function is expected to have args (observation, configuration). Here\nwe wrap our stdin stdout setup for the agent.\n\nFormat of a submission should be:\n* submission.tar.gz\n    main.py\n        (copied from main_kaggle_submission.py)\n    main_lux-ai-2021.py\n    model.zip\n        (the model you want to run)\n    /luxai2021/\n        (copy of the 'luxai2021' folder from this repo)\n    /stable_baselines3/\n        (copy of 'stable_baselines3' folder from here https://github.com/DLR-RM/stable-baselines3)\n\"\"\"\n\nfrom subprocess import Popen, PIPE\nfrom threading  import Thread\nfrom queue import Queue, Empty\n\nimport atexit\nimport os\nimport sys\nagent_processes = [None, None]\nt = None\nq = None\ndef cleanup_process():\n    global agent_processes\n    for proc in agent_processes:\n        if proc is not None:\n            proc.kill()\ndef enqueue_output(out, queue):\n    for line in iter(out.readline, b''):\n        queue.put(line)\n    out.close()\ndef python_policy_agent(observation, configuration):\n    \"\"\"\n    Wrapper for a stdin stdout agent.\n\n    Args:\n        observation ([type]): Observation data.\n        configuration ([type]): Config data.\n\n    Returns:\n        Set of actions to perform.\n    \"\"\"\n    global agent_processes, t, q\n\n    agent_process = agent_processes[observation.player]\n    ### Do not edit ###\n    if agent_process is None:\n        if \"__raw_path__\" in configuration:\n            cwd = os.path.dirname(configuration[\"__raw_path__\"])\n        else:\n            cwd = os.path.dirname(__file__)\n        agent_process = Popen([\"python3\", \"./main_lux-ai-2021.py\"], stdin=PIPE, stdout=PIPE, stderr=PIPE, cwd=cwd)\n        agent_processes[observation.player] = agent_process\n        atexit.register(cleanup_process)\n\n        # following 4 lines from https://stackoverflow.com/questions/375427/a-non-blocking-read-on-a-subprocess-pipe-in-python\n        q = Queue()\n        t = Thread(target=enqueue_output, args=(agent_process.stderr, q))\n        t.daemon = True # thread dies with the program\n        t.start()\n    if observation.step == 0:\n        # fixes bug where updates array is shared, but the first update is agent dependent actually\n        observation[\"updates\"][0] = f\"{observation.player}\"\n    \n    # print observations to agent\n    agent_process.stdin.write((\"\\n\".join(observation[\"updates\"]) + \"\\n\").encode())\n    agent_process.stdin.flush()\n\n    # wait for data written to stdout\n    agent1res = (agent_process.stdout.readline()).decode()\n    _end_res = (agent_process.stdout.readline()).decode()\n\n    while True:\n        try:  line = q.get_nowait()\n        except Empty:\n            # no standard error received, break\n            break\n        else:\n            # standard error output received, print it out\n            print(line.decode(), file=sys.stderr, end='')\n\n    outputs = agent1res.split(\"\\n\")[0].split(\",\")\n    actions = []\n    for cmd in outputs:\n        if cmd != \"\":\n            actions.append(cmd)\n    return actions","metadata":{"execution":{"iopub.status.busy":"2021-11-20T11:39:59.658958Z","iopub.status.idle":"2021-11-20T11:39:59.65927Z","shell.execute_reply.started":"2021-11-20T11:39:59.659087Z","shell.execute_reply":"2021-11-20T11:39:59.659101Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile agent_policy.py\nfrom luxai2021.game.match_controller import ActionSequence\nimport sys\nimport time\nfrom functools import partial  # pip install functools\n\nimport numpy as np\nfrom gym import spaces\nimport copy\nimport random\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport gym\nfrom gym import wrappers\nimport time\nfrom luxai2021.env.agent import Agent\nfrom luxai2021.game.actions import *\nfrom luxai2021.game.game_constants import GAME_CONSTANTS\nfrom luxai2021.game.position import Position\n\n\n# https://codereview.stackexchange.com/questions/28207/finding-the-closest-point-to-a-list-of-points\ndef closest_node(node, nodes):\n    dist_2 = np.sum((nodes - node) ** 2, axis=1)\n    return np.argmin(dist_2)\ndef furthest_node(node, nodes):\n    dist_2 = np.sum((nodes - node) ** 2, axis=1)\n    return np.argmax(dist_2)\n\ndef smart_transfer_to_nearby(game, team, unit_id, unit, target_type_restriction=None, **kwarg):\n    \"\"\"\n    Smart-transfers from the specified unit to a nearby neighbor. Prioritizes any\n    nearby carts first, then any worker. Transfers the resource type which the unit\n    has most of. Picks which cart/worker based on choosing a target that is most-full\n    but able to take the most amount of resources.\n\n    Args:\n        team ([type]): [description]\n        unit_id ([type]): [description]\n\n    Returns:\n        Action: Returns a TransferAction object, even if the request is an invalid\n                transfer. Use TransferAction.is_valid() to check validity.\n    \"\"\"\n\n    # Calculate how much resources could at-most be transferred\n    resource_type = None\n    resource_amount = 0\n    target_unit = None\n\n    if unit != None:\n        for type, amount in unit.cargo.items():\n            if amount > resource_amount:\n                resource_type = type\n                resource_amount = amount\n\n        # Find the best nearby unit to transfer to\n        unit_cell = game.map.get_cell_by_pos(unit.pos)\n        adjacent_cells = game.map.get_adjacent_cells(unit_cell)\n\n        \n        for c in adjacent_cells:\n            for id, u in c.units.items():\n                # Apply the unit type target restriction\n                if target_type_restriction == None or u.type == target_type_restriction:\n                    if u.team == team:\n                        # This unit belongs to our team, set it as the winning transfer target\n                        # if it's the best match.\n                        if target_unit is None:\n                            target_unit = u\n                        else:\n                            # Compare this unit to the existing target\n                            if target_unit.type == u.type:\n                                # Transfer to the target with the least capacity, but can accept\n                                # all of our resources\n                                if( u.get_cargo_space_left() >= resource_amount and \n                                    target_unit.get_cargo_space_left() >= resource_amount ):\n                                    # Both units can accept all our resources. Prioritize one that is most-full.\n                                    if u.get_cargo_space_left() < target_unit.get_cargo_space_left():\n                                        # This new target it better, it has less space left and can take all our\n                                        # resources\n                                        target_unit = u\n                                    \n                                elif( target_unit.get_cargo_space_left() >= resource_amount ):\n                                    # Don't change targets. Current one is best since it can take all\n                                    # the resources, but new target can't.\n                                    pass\n                                    \n                                elif( u.get_cargo_space_left() > target_unit.get_cargo_space_left() ):\n                                    # Change targets, because neither target can accept all our resources and \n                                    # this target can take more resources.\n                                    target_unit = u\n                            elif u.type == Constants.UNIT_TYPES.CART:\n                                # Transfer to this cart instead of the current worker target\n                                target_unit = u\n    \n    # Build the transfer action request\n    target_unit_id = None\n    if target_unit is not None:\n        target_unit_id = target_unit.id\n\n        # Update the transfer amount based on the room of the target\n        if target_unit.get_cargo_space_left() < resource_amount:\n            resource_amount = target_unit.get_cargo_space_left()\n    \n    return TransferAction(team, unit_id, target_unit_id, resource_type, resource_amount)\n\n########################################################################################################################\n# This is the Agent that you need to design for the competition\n########################################################################################################################\nclass AgentPolicy(Agent):\n    def __init__(self, mode=\"train\", model=None) -> None:\n        \"\"\"\n        Arguments:\n            mode: \"train\" or \"inference\", which controls if this agent is for training or not.\n            model: The pretrained model, or if None it will operate in training mode.\n        \"\"\"\n        super().__init__()\n        self.model = model\n        self.mode = mode\n        \n        self.stats = None\n        self.stats_last_game = None\n\n        # Define action and observation space\n        # They must be gym.spaces objects\n        # Example when using discrete actions:\n        self.actionSpaceMapUnits = [\n            partial(MoveAction, direction=Constants.DIRECTIONS.CENTER),  # This is the do-nothing action\n            partial(MoveAction, direction=Constants.DIRECTIONS.NORTH),\n            partial(MoveAction, direction=Constants.DIRECTIONS.WEST),\n            partial(MoveAction, direction=Constants.DIRECTIONS.SOUTH),\n            partial(MoveAction, direction=Constants.DIRECTIONS.EAST),\n            smart_transfer_to_nearby, # Transfer to nearby\n            SpawnCityAction,\n            #PillageAction,\n        ]\n        self.actionSpaceMapCities = [\n            SpawnWorkerAction,\n            SpawnCartAction,\n            ResearchAction,\n        ]\n\n        self.action_space = spaces.Discrete(max(len(self.actionSpaceMapUnits), len(self.actionSpaceMapCities)))\n        \n\n        # Observation space: (Basic minimum for a miner agent)\n        # Object:\n        #   1x is worker\n        #   1x is cart\n        #   1x is citytile\n        #\n        #   5x direction_nearest_wood\n        #   1x distance_nearest_wood\n        #   1x amount\n        #\n        #   5x direction_nearest_coal\n        #   1x distance_nearest_coal\n        #   1x amount\n        #\n        #   5x direction_nearest_uranium\n        #   1x distance_nearest_uranium\n        #   1x amount\n        #\n        #   5x direction_nearest_city\n        #   1x distance_nearest_city\n        #   1x amount of fuel\n        #\n        #   28x (the same as above, but direction, distance, and amount to the furthest of each)\n        #\n        #   5x direction_nearest_worker\n        #   1x distance_nearest_worker\n        #   1x amount of cargo\n        # Unit:\n        #   1x cargo size\n        # State:\n        #   1x is night\n        #   1x percent of game done\n        #   2x citytile counts [cur player, opponent]\n        #   2x worker counts [cur player, opponent]\n        #   2x cart counts [cur player, opponent]\n        #   1x research points [cur player]\n        #   1x researched coal [cur player]\n        #   1x researched uranium [cur player]\n        self.observation_shape = (3 + 7 * 5 * 2 + 1 + 1 + 1 + 2 + 2 + 2 + 3,)\n        self.observation_space = spaces.Box(low=0, high=1, shape=\n        self.observation_shape, dtype=np.float16)\n\n        self.object_nodes = {}\n\n    def get_agent_type(self):\n        \"\"\"\n        Returns the type of agent. Use AGENT for inference, and LEARNING for training a model.\n        \"\"\"\n        if self.mode == \"train\":\n            return Constants.AGENT_TYPE.LEARNING\n        else:\n            return Constants.AGENT_TYPE.AGENT\n\n    def get_observation(self, game, unit, city_tile, team, is_new_turn):\n        \"\"\"\n        Implements getting a observation from the current game for this unit or city\n        \"\"\"\n        observation_index = 0\n        if is_new_turn:\n            # It's a new turn this event. This flag is set True for only the first observation from each turn.\n            # Update any per-turn fixed observation space that doesn't change per unit/city controlled.\n\n            # Build a list of object nodes by type for quick distance-searches\n            self.object_nodes = {}\n\n            # Add resources\n            for cell in game.map.resources:\n                if cell.resource.type not in self.object_nodes:\n                    self.object_nodes[cell.resource.type] = np.array([[cell.pos.x, cell.pos.y]])\n                else:\n                    self.object_nodes[cell.resource.type] = np.concatenate(\n                        (\n                            self.object_nodes[cell.resource.type],\n                            [[cell.pos.x, cell.pos.y]]\n                        ),\n                        axis=0\n                    )\n\n            # Add your own and opponent units\n            for t in [team, (team + 1) % 2]:\n                for u in game.state[\"teamStates\"][team][\"units\"].values():\n                    key = str(u.type)\n                    if t != team:\n                        key = str(u.type) + \"_opponent\"\n\n                    if key not in self.object_nodes:\n                        self.object_nodes[key] = np.array([[u.pos.x, u.pos.y]])\n                    else:\n                        self.object_nodes[key] = np.concatenate(\n                            (\n                                self.object_nodes[key],\n                                [[u.pos.x, u.pos.y]]\n                            )\n                            , axis=0\n                        )\n\n            # Add your own and opponent cities\n            for city in game.cities.values():\n                for cells in city.city_cells:\n                    key = \"city\"\n                    if city.team != team:\n                        key = \"city_opponent\"\n\n                    if key not in self.object_nodes:\n                        self.object_nodes[key] = np.array([[cells.pos.x, cells.pos.y]])\n                    else:\n                        self.object_nodes[key] = np.concatenate(\n                            (\n                                self.object_nodes[key],\n                                [[cells.pos.x, cells.pos.y]]\n                            )\n                            , axis=0\n                        )\n\n        # Observation space: (Basic minimum for a miner agent)\n        # Object:\n        #   1x is worker\n        #   1x is cart\n        #   1x is citytile\n        #   5x direction_nearest_wood\n        #   1x distance_nearest_wood\n        #   1x amount\n        #\n        #   5x direction_nearest_coal\n        #   1x distance_nearest_coal\n        #   1x amount\n        #\n        #   5x direction_nearest_uranium\n        #   1x distance_nearest_uranium\n        #   1x amount\n        #\n        #   5x direction_nearest_city\n        #   1x distance_nearest_city\n        #   1x amount of fuel\n        #\n        #   5x direction_nearest_worker\n        #   1x distance_nearest_worker\n        #   1x amount of cargo\n        #\n        #   28x (the same as above, but direction, distance, and amount to the furthest of each)\n        #\n        # Unit:\n        #   1x cargo size\n        # State:\n        #   1x is night\n        #   1x percent of game done\n        #   2x citytile counts [cur player, opponent]\n        #   2x worker counts [cur player, opponent]\n        #   2x cart counts [cur player, opponent]\n        #   1x research points [cur player]\n        #   1x researched coal [cur player]\n        #   1x researched uranium [cur player]\n        obs = np.zeros(self.observation_shape)\n        \n        # Update the type of this object\n        #   1x is worker\n        #   1x is cart\n        #   1x is citytile\n        observation_index = 0\n        if unit is not None:\n            if unit.type == Constants.UNIT_TYPES.WORKER:\n                obs[observation_index] = 1.0 # Worker\n            else:\n                obs[observation_index+1] = 1.0 # Cart\n        if city_tile is not None:\n            obs[observation_index+2] = 1.0 # CityTile\n        observation_index += 3\n        \n        pos = None\n        if unit is not None:\n            pos = unit.pos\n        else:\n            pos = city_tile.pos\n\n        if pos is None:\n            observation_index += 7 * 5 * 2\n        else:\n            # Encode the direction to the nearest objects\n            #   5x direction_nearest\n            #   1x distance\n            for distance_function in [closest_node, furthest_node]:\n                for key in [\n                    Constants.RESOURCE_TYPES.WOOD,\n                    Constants.RESOURCE_TYPES.COAL,\n                    Constants.RESOURCE_TYPES.URANIUM,\n                    \"city\",\n                    str(Constants.UNIT_TYPES.WORKER)]:\n                    # Process the direction to and distance to this object type\n\n                    # Encode the direction to the nearest object (excluding itself)\n                    #   5x direction\n                    #   1x distance\n                    if key in self.object_nodes:\n                        if (\n                                (key == \"city\" and city_tile is not None) or\n                                (unit is not None and str(unit.type) == key and len(game.map.get_cell_by_pos(unit.pos).units) <= 1 )\n                        ):\n                            # Filter out the current unit from the closest-search\n                            closest_index = closest_node((pos.x, pos.y), self.object_nodes[key])\n                            filtered_nodes = np.delete(self.object_nodes[key], closest_index, axis=0)\n                        else:\n                            filtered_nodes = self.object_nodes[key]\n\n                        if len(filtered_nodes) == 0:\n                            # No other object of this type\n                            obs[observation_index + 5] = 1.0\n                        else:\n                            # There is another object of this type\n                            closest_index = distance_function((pos.x, pos.y), filtered_nodes)\n\n                            if closest_index is not None and closest_index >= 0:\n                                closest = filtered_nodes[closest_index]\n                                closest_position = Position(closest[0], closest[1])\n                                direction = pos.direction_to(closest_position)\n                                mapping = {\n                                    Constants.DIRECTIONS.CENTER: 0,\n                                    Constants.DIRECTIONS.NORTH: 1,\n                                    Constants.DIRECTIONS.WEST: 2,\n                                    Constants.DIRECTIONS.SOUTH: 3,\n                                    Constants.DIRECTIONS.EAST: 4,\n                                }\n                                obs[observation_index + mapping[direction]] = 1.0  # One-hot encoding direction\n\n                                # 0 to 1 distance\n                                distance = pos.distance_to(closest_position)\n                                obs[observation_index + 5] = min(distance / 20.0, 1.0)\n\n                                # 0 to 1 value (amount of resource, cargo for unit, or fuel for city)\n                                if key == \"city\":\n                                    # City fuel as % of upkeep for 200 turns\n                                    c = game.cities[game.map.get_cell_by_pos(closest_position).city_tile.city_id]\n                                    obs[observation_index + 6] = min(\n                                        c.fuel / (c.get_light_upkeep() * 200.0),\n                                        1.0\n                                    )\n                                elif key in [Constants.RESOURCE_TYPES.WOOD, Constants.RESOURCE_TYPES.COAL,\n                                             Constants.RESOURCE_TYPES.URANIUM]:\n                                    # Resource amount\n                                    obs[observation_index + 6] = min(\n                                        game.map.get_cell_by_pos(closest_position).resource.amount / 500,\n                                        1.0\n                                    )\n                                else:\n                                    # Unit cargo\n                                    obs[observation_index + 6] = min(\n                                        next(iter(game.map.get_cell_by_pos(\n                                            closest_position).units.values())).get_cargo_space_left() / 100,\n                                        1.0\n                                    )\n\n                    observation_index += 7\n\n        if unit is not None:\n            # Encode the cargo space\n            #   1x cargo size\n            obs[observation_index] = unit.get_cargo_space_left() / GAME_CONSTANTS[\"PARAMETERS\"][\"RESOURCE_CAPACITY\"][\n                \"WORKER\"]\n            observation_index += 1\n        else:\n            observation_index += 1\n\n        # Game state observations\n\n        #   1x is night\n        obs[observation_index] = game.is_night()\n        observation_index += 1\n\n        #   1x percent of game done\n        obs[observation_index] = game.state[\"turn\"] / GAME_CONSTANTS[\"PARAMETERS\"][\"MAX_DAYS\"]\n        observation_index += 1\n\n        #   2x citytile counts [cur player, opponent]\n        #   2x worker counts [cur player, opponent]\n        #   2x cart counts [cur player, opponent]\n        max_count = 30\n        for key in [\"city\", str(Constants.UNIT_TYPES.WORKER), str(Constants.UNIT_TYPES.CART)]:\n            if key in self.object_nodes:\n                obs[observation_index] = len(self.object_nodes[key]) / max_count\n            if (key + \"_opponent\") in self.object_nodes:\n                obs[observation_index + 1] = len(self.object_nodes[(key + \"_opponent\")]) / max_count\n            observation_index += 2\n\n        #   1x research points [cur player]\n        #   1x researched coal [cur player]\n        #   1x researched uranium [cur player]\n        obs[observation_index] = game.state[\"teamStates\"][team][\"researchPoints\"] / 200.0\n        obs[observation_index+1] = float(game.state[\"teamStates\"][team][\"researched\"][\"coal\"])\n        obs[observation_index+2] = float(game.state[\"teamStates\"][team][\"researched\"][\"uranium\"])\n\n        return obs\n\n    def action_code_to_action(self, action_code, game, unit=None, city_tile=None, team=None):\n        \"\"\"\n        Takes an action in the environment according to actionCode:\n            actionCode: Index of action to take into the action array.\n        Returns: An action.\n        \"\"\"\n        # Map actionCode index into to a constructed Action object\n        try:\n            x = None\n            y = None\n            if city_tile is not None:\n                x = city_tile.pos.x\n                y = city_tile.pos.y\n            elif unit is not None:\n                x = unit.pos.x\n                y = unit.pos.y\n            \n            if city_tile != None:\n                action =  self.actionSpaceMapCities[action_code%len(self.actionSpaceMapCities)](\n                    game=game,\n                    unit_id=unit.id if unit else None,\n                    unit=unit,\n                    city_id=city_tile.city_id if city_tile else None,\n                    citytile=city_tile,\n                    team=team,\n                    x=x,\n                    y=y\n                )\n\n                # If the city action is invalid, default to research action automatically\n                if not action.is_valid(game, actions_validated=[]):\n                    action = ResearchAction(\n                        game=game,\n                        unit_id=unit.id if unit else None,\n                        unit=unit,\n                        city_id=city_tile.city_id if city_tile else None,\n                        citytile=city_tile,\n                        team=team,\n                        x=x,\n                        y=y\n                    )\n            else:\n                action =  self.actionSpaceMapUnits[action_code%len(self.actionSpaceMapUnits)](\n                    game=game,\n                    unit_id=unit.id if unit else None,\n                    unit=unit,\n                    city_id=city_tile.city_id if city_tile else None,\n                    citytile=city_tile,\n                    team=team,\n                    x=x,\n                    y=y\n                )\n            \n            return action\n        except Exception as e:\n            # Not a valid action\n            print(e)\n            return None\n\n    def take_action(self, action_code, game, unit=None, city_tile=None, team=None):\n        \"\"\"\n        Takes an action in the environment according to actionCode:\n            actionCode: Index of action to take into the action array.\n        \"\"\"\n        action = self.action_code_to_action(action_code, game, unit, city_tile, team)\n        self.match_controller.take_action(action)\n    \n    def game_start(self, game):\n        \"\"\"\n        This funciton is called at the start of each game. Use this to\n        reset and initialize per game. Note that self.team may have\n        been changed since last game. The game map has been created\n        and starting units placed.\n\n        Args:\n            game ([type]): Game.\n        \"\"\"\n        self.last_generated_fuel = game.stats[\"teamStats\"][self.team][\"fuelGenerated\"]\n        self.last_resources_collected = copy.deepcopy(game.stats[\"teamStats\"][self.team][\"resourcesCollected\"])\n        if self.stats != None:\n            self.stats_last_game =  self.stats\n        self.stats = {\n            \"rew/r_total\": 0,\n            \"rew/r_wood\": 0,\n            \"rew/r_coal\": 0,\n            \"rew/r_uranium\": 0,\n            \"rew/r_research\": 0,\n            \"rew/r_city_tiles_end\": 0,\n            \"rew/r_fuel_collected\":0,\n            \"rew/r_units\":0,\n            \"rew/r_city_tiles\":0,\n            \"game/turns\": 0,\n            \"game/research\": 0,\n            \"game/unit_count\": 0,\n            \"game/cart_count\": 0,\n            \"game/city_count\": 0,\n            \"game/city_tiles\": 0,\n            \"game/wood_rate_mined\": 0,\n            \"game/coal_rate_mined\": 0,\n            \"game/uranium_rate_mined\": 0,\n        }\n        self.is_last_turn = False\n\n        # Calculate starting map resources\n        type_map = {\n            Constants.RESOURCE_TYPES.WOOD: \"WOOD\",\n            Constants.RESOURCE_TYPES.COAL: \"COAL\",\n            Constants.RESOURCE_TYPES.URANIUM: \"URANIUM\",\n        }\n\n        self.fuel_collected_last = 0\n        self.fuel_start = {}\n        self.fuel_last = {}\n        for type, type_upper in type_map.items():\n            self.fuel_start[type] = 0\n            self.fuel_last[type] = 0\n            for c in game.map.resources_by_type[type]:\n                self.fuel_start[type] += c.resource.amount * game.configs[\"parameters\"][\"RESOURCE_TO_FUEL_RATE\"][type_upper]\n\n        self.research_last = 0\n        self.units_last = 0\n        self.city_tiles_last = 0\n\n    def get_reward(self, game, is_game_finished, is_new_turn, is_game_error):\n        \"\"\"\n        Returns the reward function for this step of the game.\n        \"\"\"\n        if is_game_error:\n            # Game environment step failed, assign a game lost reward to not incentivise this\n            print(\"Game failed due to error\")\n            return -1.0\n\n        if not is_new_turn and not is_game_finished:\n            # Only apply rewards at the start of each turn\n            return 0\n\n        # Get some basic stats\n        unit_count = len(game.state[\"teamStates\"][self.team % 2][\"units\"])\n        cart_count = 0\n        for id, u in game.state[\"teamStates\"][self.team % 2][\"units\"].items():\n            if u.type == Constants.UNIT_TYPES.CART:\n                cart_count += 1\n\n        unit_count_opponent = len(game.state[\"teamStates\"][(self.team + 1) % 2][\"units\"])\n        research = min(game.state[\"teamStates\"][self.team][\"researchPoints\"], 200.0) # Cap research points at 200\n        city_count = 0\n        city_count_opponent = 0\n        city_tile_count = 0\n        city_tile_count_opponent = 0\n        for city in game.cities.values():\n            if city.team == self.team:\n                city_count += 1\n            else:\n                city_count_opponent += 1\n\n            for cell in city.city_cells:\n                if city.team == self.team:\n                    city_tile_count += 1\n                else:\n                    city_tile_count_opponent += 1\n        \n        # Basic stats\n        self.stats[\"game/research\"] = research\n        self.stats[\"game/city_tiles\"] = city_tile_count\n        self.stats[\"game/city_count\"] = city_count\n        self.stats[\"game/unit_count\"] = unit_count\n        self.stats[\"game/cart_count\"] = cart_count\n        self.stats[\"game/turns\"] = game.state[\"turn\"]\n\n        rewards = {}\n\n        # Give up to 1.0 reward for each resource based on % of total mined.\n        type_map = {\n            Constants.RESOURCE_TYPES.WOOD: \"WOOD\",\n            Constants.RESOURCE_TYPES.COAL: \"COAL\",\n            Constants.RESOURCE_TYPES.URANIUM: \"URANIUM\",\n        }\n        fuel_now = {}\n        for type, type_upper in type_map.items():\n            fuel_now = game.stats[\"teamStats\"][self.team][\"resourcesCollected\"][type] * game.configs[\"parameters\"][\"RESOURCE_TO_FUEL_RATE\"][type_upper]\n            rewards[\"rew/r_%s\" % type] = (fuel_now - self.fuel_last[type]) / self.fuel_start[type]\n            self.stats[\"game/%s_rate_mined\" % type] = fuel_now / self.fuel_start[type]\n            self.fuel_last[type] = fuel_now\n        \n        # Give more incentive for coal and uranium\n        rewards[\"rew/r_%s\" % Constants.RESOURCE_TYPES.COAL] *= 2\n        rewards[\"rew/r_%s\" % Constants.RESOURCE_TYPES.URANIUM] *= 4\n        \n        # Give a reward based on amount of fuel collected. 1.0 reward for each 20K fuel gathered.\n        fuel_collected = game.stats[\"teamStats\"][self.team][\"fuelGenerated\"]\n        rewards[\"rew/r_fuel_collected\"] = ( (fuel_collected - self.fuel_collected_last) / 20000 )\n        self.fuel_collected_last = fuel_collected\n\n        # Give a reward for unit creation/death. 0.05 reward per unit.\n        rewards[\"rew/r_units\"] = (unit_count - self.units_last) * 0.05\n        self.units_last = unit_count\n\n        # Give a reward for unit creation/death. 0.1 reward per city.\n        rewards[\"rew/r_city_tiles\"] = (city_tile_count - self.city_tiles_last) * 0.1\n        self.city_tiles_last = city_tile_count\n\n        # Tiny reward for research to help. Up to 0.5 reward for this.\n        rewards[\"rew/r_research\"] = (research - self.research_last) / (200 * 2)\n        self.research_last = research\n        \n        # Give a reward up to around 50.0 based on number of city tiles at the end of the game\n        rewards[\"rew/r_city_tiles_end\"] = 0\n        if is_game_finished:\n            self.is_last_turn = True\n            rewards[\"rew/r_city_tiles_end\"] = city_tile_count\n        \n        \n        # Update the stats and total reward\n        reward = 0\n        for name, value in rewards.items():\n            self.stats[name] += value\n            reward += value\n        self.stats[\"rew/r_total\"] += reward\n\n        # Print the final game stats sometimes\n        if is_game_finished and random.random() <= 0.15:\n            stats_string = []\n            for key, value in self.stats.items():\n                stats_string.append(\"%s=%.2f\" % (key, value))\n            print(\",\".join(stats_string))\n\n\n        return reward\n        \n    \n\n    def process_turn(self, game, team):\n        \"\"\"\n        Decides on a set of actions for the current turn. Not used in training, only inference.\n        Returns: Array of actions to perform.\n        \"\"\"\n        start_time = time.time()\n        actions = []\n        new_turn = True\n\n        # Inference the model per-unit\n        units = game.state[\"teamStates\"][team][\"units\"].values()\n        for unit in units:\n            if unit.can_act():\n                obs = self.get_observation(game, unit, None, unit.team, new_turn)\n                obs = torch.from_numpy(obs).float().view(1, 85)\n                action_code, _states = self.model.predict(obs, deterministic=False)\n                if action_code is not None:\n                    actions.append(\n                        self.action_code_to_action(action_code, game=game, unit=unit, city_tile=None, team=unit.team))\n                new_turn = False\n\n        # Inference the model per-city\n        cities = game.cities.values()\n        for city in cities:\n            if city.team == team:\n                for cell in city.city_cells:\n                    city_tile = cell.city_tile\n                    if city_tile.can_act():\n                        obs = self.get_observation(game, None, city_tile, city.team, new_turn)\n                        obs = torch.from_numpy(obs).float().view(1, 85)\n                        action_code, _states = self.model.predict(obs, deterministic=False)\n                        if action_code is not None:\n                            actions.append(\n                                self.action_code_to_action(action_code, game=game, unit=None, city_tile=city_tile,\n                                                           team=city.team))\n                        new_turn = False\n\n        time_taken = time.time() - start_time\n        if time_taken > 0.5:  # Warn if larger than 0.5 seconds.\n            print(\"WARNING: Inference took %.3f seconds for computing actions. Limit is 1 second.\" % time_taken,\n                  file=sys.stderr)\n\n        return actions\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-20T11:39:59.661012Z","iopub.status.idle":"2021-11-20T11:39:59.661531Z","shell.execute_reply.started":"2021-11-20T11:39:59.661346Z","shell.execute_reply":"2021-11-20T11:39:59.661369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp tensor.pt kaggle_submissions/\n!cp agent_policy.py kaggle_submissions/\n!cp main_lux-ai-2021.py kaggle_submissions/\n!cp main.py kaggle_submissions/\n!cp -R luxai2021 kaggle_submissions/\n!cp -R stable_baselines3 kaggle_submissions/","metadata":{"execution":{"iopub.status.busy":"2021-11-20T11:39:59.663183Z","iopub.status.idle":"2021-11-20T11:39:59.66352Z","shell.execute_reply.started":"2021-11-20T11:39:59.663345Z","shell.execute_reply":"2021-11-20T11:39:59.663369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -czf submission.tar.gz -C kaggle_submissions .","metadata":{"id":"ZfPsLITHpLfY","outputId":"679435f7-17a0-4cd6-be57-b4b51e4ec235","execution":{"iopub.status.busy":"2021-11-20T11:39:59.664557Z","iopub.status.idle":"2021-11-20T11:39:59.664859Z","shell.execute_reply.started":"2021-11-20T11:39:59.664701Z","shell.execute_reply":"2021-11-20T11:39:59.664722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from kaggle_environments import make\nimport json\n# run another match but with our empty agent\nenv = make(\"lux_ai_2021\", configuration={\"seed\": 562124210, \"loglevel\": 2, \"annotations\": True}, debug=True)\n\n# Play the environment where the RL agent plays against itself\nsteps = env.run([\"./kaggle_submissions/main.py\", \"./kaggle_submissions/main.py\"])\nenv.render(mode=\"ipython\", width=1200, height=800)","metadata":{"id":"ZfPsLITHpLfY","outputId":"679435f7-17a0-4cd6-be57-b4b51e4ec235","execution":{"iopub.status.busy":"2021-11-20T11:39:59.665578Z","iopub.status.idle":"2021-11-20T11:39:59.665888Z","shell.execute_reply.started":"2021-11-20T11:39:59.665736Z","shell.execute_reply":"2021-11-20T11:39:59.665752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!ls kaggle_submissions","metadata":{"execution":{"iopub.status.busy":"2021-11-20T11:39:59.666956Z","iopub.status.idle":"2021-11-20T11:39:59.667263Z","shell.execute_reply.started":"2021-11-20T11:39:59.667087Z","shell.execute_reply":"2021-11-20T11:39:59.667102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}